{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b799fe99-4555-4738-a397-632395b14af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import hashlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch.nn.functional as nnF\n",
    "from datetime import date, datetime\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.feature_extraction.image import img_to_graph\n",
    "from scipy.sparse import coo_matrix\n",
    "import os.path as osp\n",
    "\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import Sequential as GNNSequential\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8002f8a-119b-4167-8a93-705f9916fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(234213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d5c56fdaaa57b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T01:59:16.479743Z",
     "start_time": "2024-09-26T01:59:15.198366Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# collecting training image paths and names into lists\n",
    "training_images_output_path = \"../../replicatingAlexNet/images/training_images_processed/\"\n",
    "\n",
    "training_images_processed_path = [\n",
    "    training_images_output_path + f for f in os.listdir(training_images_output_path)\n",
    "    if f.endswith('.JPEG')\n",
    "]\n",
    "\n",
    "training_images_processed_names = [\n",
    "    f for f in os.listdir(training_images_output_path)\n",
    "    if f.endswith('.JPEG')\n",
    "]\n",
    "\n",
    "# collecting validation image paths and names into lists\n",
    "validation_images_output_path = \"../../replicatingAlexNet/images/validation_images_processed/\"\n",
    "\n",
    "validation_images_processed_path = [\n",
    "    validation_images_output_path + f for f in os.listdir(validation_images_output_path)\n",
    "    if f.endswith('.JPEG')\n",
    "]\n",
    "\n",
    "validation_images_processed_names = [\n",
    "    f for f in os.listdir(validation_images_output_path)\n",
    "    if f.endswith('.JPEG')\n",
    "]\n",
    "\n",
    "# sorting names in ascending order; this is crucial to ensure that the\n",
    "# proper validation labels are attached to the file names\n",
    "validation_images_processed_names = sorted(validation_images_processed_names)\n",
    "\n",
    "labels_w_information = pl.read_csv(\"../../replicatingAlexNet/images/labels_w_information.csv\")\n",
    "training_labels_for_CEloss = labels_w_information[\"made_up_label\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af5c54c957c9a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T01:59:18.189734Z",
     "start_time": "2024-09-26T01:59:17.055593Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196608, 196608)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    img_to_graph(\n",
    "        F.pil_to_tensor(Image.open(training_images_processed_path[0])), \n",
    "        return_as=np.ndarray\n",
    "    ).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4707d276c4e52ee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T01:59:18.414480Z",
     "start_time": "2024-09-26T01:59:18.240938Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# USING SOME PREVIOUSLY WRITTEN CODE\n",
    "\n",
    "# below, I am sampling only labels with greater than 1200 examples and then randomly selecting 25 labels\n",
    "labels_thousand_plus = [\n",
    "    label for label, count in Counter(training_labels_for_CEloss).items() \n",
    "    if count >= 1200\n",
    "]\n",
    "labels_thousand_plus_subset = random.sample(labels_thousand_plus, k=25)\n",
    "\n",
    "# mapping the subset to numbers between 0-24\n",
    "# this is needed for the CE loss function\n",
    "labels_subset_mapped = {\n",
    "    original_label: new_label for original_label, new_label\n",
    "    in zip(labels_thousand_plus_subset, range(0, len(labels_thousand_plus_subset)))\n",
    "}\n",
    "\n",
    "# a dictionary of indices as keys with the 25 randomly selected labels as values\n",
    "label_w_index = {\n",
    "    index: label for index, label \n",
    "    in zip(range(0, len(training_labels_for_CEloss)), training_labels_for_CEloss)\n",
    "    if label in labels_thousand_plus_subset\n",
    "}\n",
    "\n",
    "# randomly selecting k of those indices which are then used to \n",
    "# select the subset of inputs and labels in training_subset\n",
    "random_indices = random.sample(list(label_w_index.keys()), k=int(256 * 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999b0be67c40e51e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# loading the validation IDs provided in the Image Net devkit\n",
    "validation_ILSVRC2010_IDs = []\n",
    "\n",
    "with open(\"../../replicatingAlexNet/devkit-1.0/data/ILSVRC2010_validation_ground_truth.txt\") as file:\n",
    "    while line := file.readline():\n",
    "        validation_ILSVRC2010_IDs.append(int(line.rstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a27f49-dd1c-439b-a4f0-69740fb5bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the IDs to labels for the validation data\n",
    "validation_true_labels = [i - 1 for i in validation_ILSVRC2010_IDs]\n",
    "\n",
    "validation_subset_25_labels = {\n",
    "    image_name: label for image_name, label \n",
    "    in zip(validation_images_processed_names, validation_true_labels) \n",
    "    if label in labels_thousand_plus_subset\n",
    "}\n",
    "\n",
    "# sampling based on the random indices but storing \n",
    "# the 0-24 values using the labels_subset_mapped dictionary\n",
    "training_labels_subset_sampled_converted = [\n",
    "    labels_subset_mapped.get(training_labels_for_CEloss[i]) for i in random_indices\n",
    "]\n",
    "\n",
    "# sampling image names based on the random indices\n",
    "training_image_names_subset_sampled = [\n",
    "    labels_w_information[\"image_name\"][i] for i in random_indices\n",
    "]\n",
    "\n",
    "validation_labels_subset_sampled_converted = [\n",
    "    labels_subset_mapped.get(i) for i in list(validation_subset_25_labels.values())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee04e454-167c-412c-bcad-8b759be8f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    img_to_graph(\n",
    "        F.pil_to_tensor(Image.open(f\"{training_images_output_path}/{i}\")), \n",
    "        return_as=np.ndarray\n",
    "    ) for i in training_image_names_subset_sampled[:1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a1bc40b-d508-4f17-a150-e4aeb07c936e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,   1,   0, ...,   0,   0,   0],\n",
       "       [  1,  66,   1, ...,   0,   0,   0],\n",
       "       [  0,   1,  65, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 151, 232,   0],\n",
       "       [  0,   0,   0, ..., 232, 175, 248],\n",
       "       [  0,   0,   0, ...,   0, 248, 183]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd30575-8b2d-421b-bf82-09497c5a8b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated total size of 0.00003th of the augmented dataset is 36.00 gigabytes.\n",
      "The estimated total size of the augmented training set is 1152.00 terabytes.\n"
     ]
    }
   ],
   "source": [
    "size_in_bytes = torch.vstack((torch.from_numpy(test_list[0]),)).element_size() \\\n",
    "                * torch.vstack((torch.from_numpy(test_list[0]),)).nelement()\n",
    "size_in_gbs = size_in_bytes / (1024 ** 3)\n",
    "\n",
    "print(\n",
    "    f\"The estimated total size of {(1 / len(training_image_names_subset_sampled)):.5f}th \"\n",
    "    f\"of the augmented dataset is {(1 * size_in_gbs):.2f} gigabytes.\"\n",
    ")\n",
    "print(\n",
    "    f\"The estimated total size of the augmented training set is \"\n",
    "    f\"{(len(training_image_names_subset_sampled) * size_in_gbs / 1024):.2f} terabytes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad8f042-43e9-4161-a6dd-e614c06a4d32",
   "metadata": {},
   "source": [
    "oh no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c98d33-2270-45fd-a764-a6e46864311b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196608, 196608)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e0eaaf-5060-42f2-8df2-2700dfba2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = Image.open(str(training_images_output_path + training_image_names_subset_sampled[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de21f734-3dc0-44b0-ac6a-14aafe89e7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1 = im1.resize((64, 64))\n",
    "im1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb49f702-a360-434e-adeb-b91b788242ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64e40f03-0131-44a0-8483-dbb13b1053b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 12288)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_to_graph(\n",
    "    F.pil_to_tensor(im1), \n",
    "    return_as=np.ndarray\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10299d56-9c0f-46e6-afc8-7dca4e87cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated total size of 0.00003th of the augmented dataset is 0.14 gigabytes.\n",
      "The estimated total size of the augmented training set is 4.50 terabytes.\n"
     ]
    }
   ],
   "source": [
    "size_in_bytes = torch.vstack((torch.from_numpy(img_to_graph(F.pil_to_tensor(im1), return_as=np.ndarray)),)).element_size() \\\n",
    "                * torch.vstack((torch.from_numpy(img_to_graph(F.pil_to_tensor(im1), return_as=np.ndarray)),)).nelement()\n",
    "size_in_gbs = size_in_bytes / (1024 ** 3)\n",
    "\n",
    "print(\n",
    "    f\"The estimated total size of {(1 / len(training_image_names_subset_sampled)):.5f}th \"\n",
    "    f\"of the augmented dataset is {(1 * size_in_gbs):.2f} gigabytes.\"\n",
    ")\n",
    "print(\n",
    "    f\"The estimated total size of the augmented training set is \"\n",
    "    f\"{(len(training_image_names_subset_sampled) * size_in_gbs / 1024):.2f} terabytes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20b7e9-f830-48d2-a4b5-8079461ba5ac",
   "metadata": {},
   "source": [
    "Ok, much better. **Now, the question is: how bad is the loss of information from such a drastic size reduction?**\n",
    "\n",
    "It isn't ideal that in addition to the input format for the graph model (just the adjacency matrix), there is another variable of uncertainty (a vastly reduced matrix size compared to one from a 256 x 256 image).\n",
    "\n",
    "It may be better to _sparsify_ the adjacency matrix of the full image to reduce its size, but for now, I will trudge along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fb82e9-eb07-4ebc-8f8f-7ea1580fbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_to_graphs(\n",
    "    file_name\n",
    "):\n",
    "    image = Image.open(file_name)\n",
    "    image = image.resize((64, 64))\n",
    "    graph = torch.from_numpy(img_to_graph(F.pil_to_tensor(im1), return_as=np.ndarray)).to(\"mps\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68130504-e690-4538-b7fe-c2842f1f61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[process_images_to_graphs(training_images_output_path + i) for i in training_image_names_subset_sampled[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305a3ac8-de4c-40dd-9246-cd10b097cc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 * 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d79b96c-b2c2-4b22-ae12-977e53cb237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGraphDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        image_names, \n",
    "        labels, \n",
    "        transform=None, \n",
    "        pre_transform=None, \n",
    "        pre_filter=None\n",
    "    ):\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.root = root\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.image_names\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{i}.pt' for i in range(len(self.image_names))]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        for i, (image_name, label) in enumerate(zip(self.image_names, self.labels)):\n",
    "            raw_path = osp.join(self.root, image_name)\n",
    "            x, edge_index, edge_attr = self.process_image_to_graph(raw_path)\n",
    "            \n",
    "            data = Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=torch.tensor([label], dtype=torch.long)\n",
    "            )\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{i}.pt'))\n",
    "\n",
    "    def process_image_to_graph(self, file_name):\n",
    "        image = Image.open(file_name)\n",
    "        image = image.resize((64, 64))\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # creating graph from image\n",
    "        graph = img_to_graph(image_array)\n",
    "        \n",
    "        # converting to COO format\n",
    "        coo = coo_matrix(graph)\n",
    "        \n",
    "        # creating node features, edge indices, and weights\n",
    "        x = torch.from_numpy(image_array.reshape(-1, 1)).float()\n",
    "        edge_index = torch.from_numpy(np.vstack((coo.row, coo.col))).long()\n",
    "        edge_attr = torch.from_numpy(coo.data).float().unsqueeze(1)\n",
    "        \n",
    "        return x, edge_index, edge_attr\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'), weights_only=False)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a308ef5-1054-42b9-ae72-6a75d985d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageGraphDataset(\n",
    "    root=training_images_output_path,\n",
    "    image_names=training_image_names_subset_sampled,\n",
    "    labels=training_labels_subset_sampled_converted\n",
    ")\n",
    "\n",
    "val_dataset = ImageGraphDataset(\n",
    "    root=validation_images_output_path,\n",
    "    image_names=list(validation_subset_25_labels.keys()),\n",
    "    labels=validation_labels_subset_sampled_converted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd48e0-5138-4c4b-a5e3-2afd8b3b979d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e65bda9-403d-407f-90a8-c0413878329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN designed by Claude based on my AlexNet implementation\n",
    "\n",
    "class AlexNetInspiredGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, num_classes):\n",
    "        super(AlexNetInspiredGNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features, 96)\n",
    "        self.pool1 = TopKPooling(96, ratio=0.5)\n",
    "        \n",
    "        self.conv2 = GCNConv(96, 256)\n",
    "        self.pool2 = TopKPooling(256, ratio=0.5)\n",
    "        \n",
    "        self.conv3 = GCNConv(256, 384)\n",
    "        \n",
    "        self.conv4 = GCNConv(384, 384)\n",
    "        \n",
    "        self.conv5 = GCNConv(384, 256)\n",
    "        self.pool5 = TopKPooling(256, ratio=0.5)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256, 4096)\n",
    "        self.fc2 = torch.nn.Linear(4096, 4096)\n",
    "        self.fc3 = torch.nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        if len(args) == 1 and hasattr(args[0], 'x'):\n",
    "            data = args[0]\n",
    "            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        elif len(args) == 4:\n",
    "            x, edge_index, edge_attr, batch = args\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input. Expecting either a single Data object or x, edge_index, edge_attr, batch\")\n",
    "        \n",
    "        # Conv1 + Pool1\n",
    "        x = nnF.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        \n",
    "        # Conv2 + Pool2\n",
    "        x = nnF.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        \n",
    "        # Conv3\n",
    "        x = nnF.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        \n",
    "        # Conv4\n",
    "        x = nnF.relu(self.conv4(x, edge_index, edge_attr))\n",
    "        \n",
    "        # Conv5 + Pool5\n",
    "        x = nnF.relu(self.conv5(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch, _, _ = self.pool5(x, edge_index, edge_attr, batch)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = nnF.relu(self.fc1(x))\n",
    "        x = nnF.dropout(x, p=0.5, training=self.training)\n",
    "        x = nnF.relu(self.fc2(x))\n",
    "        x = nnF.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return nnF.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4830c-fad4-408b-aba4-c25053bfb3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007bef9-d861-48bf-9eb7-643d37404c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(file_name, epoch, train_loss, train_acc, val_loss, val_acc, lr):\n",
    "    with open(file_name, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch, train_loss, train_acc, val_loss, val_acc, lr])\n",
    "\n",
    "def train_and_validate(model, train_dataset, val_dataset, num_epochs=100, batch_size=32, lr=0.01, log_file_name='gnn_training_log.csv'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "\n",
    "    # create and write header to CSV file\n",
    "    with open(log_file_name, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Val Loss', 'Val Accuracy', 'Learning Rate'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training block\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * data.num_graphs\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total_train += data.y.size(0)\n",
    "            correct_train += predicted.eq(data.y).sum().item()\n",
    "\n",
    "        # computing training metrics\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct_train / total_train\n",
    "\n",
    "        # validation block\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        val_loss_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                val_out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                val_loss = criterion(val_out, data.y)\n",
    "                val_loss_count += val_loss.item() * data.num_graphs\n",
    "\n",
    "                _, val_predicted = torch.max(val_out, 1)\n",
    "                total_val += data.y.size(0)\n",
    "                correct_val += val_predicted.eq(data.y).sum().item()\n",
    "\n",
    "        # computing validation metrics\n",
    "        val_epoch_loss = val_loss_count / len(val_loader.dataset)\n",
    "        val_epoch_acc = correct_val / total_val\n",
    "\n",
    "        # learning rate scheduler\n",
    "        scheduler.step(val_epoch_acc)\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(\n",
    "            f\"{str(epoch + 1).zfill(2)}/{num_epochs}, \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}, \"\n",
    "            f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_acc:.4f}, \"\n",
    "            f\"Learning Rate: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        write_to_csv(log_file_name, epoch + 1, epoch_loss, epoch_acc, val_epoch_loss, val_epoch_acc, current_lr)\n",
    "\n",
    "    print(f\"Trial training done! Loss and accuracy values were saved to {log_file_name}\")\n",
    "\n",
    "num_node_features = 1 \n",
    "num_edge_features = 1 \n",
    "num_classes = 25  \n",
    "\n",
    "model = AlexNetInspiredGNN(num_node_features, num_edge_features, num_classes)\n",
    "train_and_validate(model, train_dataset, val_dataset, num_epochs=100, batch_size=32, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235369c-3704-411a-84f4-6ebea39efa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
